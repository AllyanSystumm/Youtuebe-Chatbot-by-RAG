# -*- coding: utf-8 -*-
"""Untitled9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_13Fxv_SP4E0yJdXqGLyTFOoCxz7b0RJ

# **INSTALL LIBRARIES**
"""

import os
!pip install groq
os.environ["HUGGINGFACE_API_KEY"] = "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"

!pip install -q youtube-transcript-api langchain-community langchain-openai \
               faiss-cpu tiktoken python-dotenv

from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate

"""Install libraries

# Step 1a - Indexing (Document Ingestion)
"""

from youtube_transcript_api import YouTubeTranscriptApi

video_id = "7rWGv8f2B2Y"

try:
    # 1. Create an instance of the API class
    ytt_api = YouTubeTranscriptApi()

    # 2. Use the .fetch() method on the instance to get a single transcript.
    # .fetch() requires a single video_id, not a list.
    # You can also pass the languages parameter here if needed.
    fetched_transcript = ytt_api.fetch(video_id, languages=['en'])

    # 3. Convert the FetchedTranscript object to the raw data format (list of dicts)
    transcript_data_for_video = fetched_transcript.to_raw_data()

    full_transcript_text = " ".join([
        item['text'] for item in transcript_data_for_video
    ])

    # Displaying the transcript as a list
    print("Transcript Data:")
    for chunk in transcript_data_for_video:
        print(f"Start: {chunk['start']}s, Duration: {chunk['duration']}s, Text: {chunk['text']}")

except Exception as e:
    print(f"Error: {e}")

"""# Step 1b - Indexing (Text Splitting)"""

from langchain_text_splitters import RecursiveCharacterTextSplitter

# 3. Initialize the text splitter
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

# 4. Split the single string (wrapped in a list) into LangChain Documents
chunks = splitter.create_documents([full_transcript_text])

# ----------------------------------------------------------------------
# UPDATED CODE END
# ----------------------------------------------------------------------

# Display results
print(f"\nâœ… Transcript successfully fetched and split.")
print(f"Total number of chunks created: {len(chunks)}")
print(f"First chunk preview (Length: {len(chunks[0].page_content)} characters)")

len(chunks)

chunks[1]

"""# Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)"""

# Install the necessary libraries
!pip install -qqq langchain-community sentence-transformers faiss-cpu

# ----------------------------------------------------------------------

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS # Note: Changed from `langchain` to `langchain_community`

# 1. Initialize the Hugging Face Embeddings
# 'all-MiniLM-L6-v2' is a highly recommended, fast, and small model.
model_name = "all-MiniLM-L6-v2"
model_kwargs = {'device': 'cpu'} # Use 'cuda' if you have a Colab GPU enabled for faster processing
encode_kwargs = {'normalize_embeddings': False}

# Initialize the HuggingFaceEmbeddings class
embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)

# 2. Create the FAISS vector store
# Assuming 'chunks' is a list of Document objects defined elsewhere
vector_store = FAISS.from_documents(chunks, embeddings)

"""# Huggingface token validation"""

from huggingface_hub import whoami

HUGGINGFACE_API_KEY = "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"

try:
    user_info = whoami(token=HUGGINGFACE_API_KEY)
    print(f"Logged in as: {user_info['name']}")
    print("Token is valid!")
except Exception as e:
    print(f"Invalid token: {e}")

vector_store.index_to_docstore_id

vector_store.get_by_ids(['4b3f8da4-7d10-4b96-b0c4-5d466c5c5a17'])

"""# Step 2 - Retrieval"""

retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})

retriever.invoke('how are you and who is brock ?')

"""# Step 3 - Augmentation

# Groq token validation and groq Model
"""

from groq import Groq

GROQ_API_KEY = "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"

try:
    client = Groq(api_key=GROQ_API_KEY)

    # # Make a minimal API call to verify the token
    # response = client.chat.completions.create(
    #     model="llama-3.3-70b-versatile",
    #     messages=[{"role": "user", "content": "Hi"}],
    #     max_tokens=5
    # )

    print("âœ“ Token is valid!")
    print(f"âœ“ Successfully connected to Groq API")
    # print(f"âœ“ Response received from model: {response.model}")

except Exception as e:
    print(f"âœ— Invalid token or API error: {e}")

# prompt = PromptTemplate(
#     template="""
#       You are a helpful assistant.
#       Answer ONLY from the provided transcript context.
#       If the context is insufficient, just say you don't know.

#       {context}
#       Question: {question}
#     """,
#     input_variables = ['context', 'question']
# )

prompt = PromptTemplate(
    template="""
You are a helpful assistant that answers questions based ONLY on the provided context from PDF documents.

CONTEXT:
{context}

INSTRUCTIONS:
1. Answer the question using ONLY the information from the context above
2. If the question is not related to the domain or topics covered in the context, respond with: "I am unable to respond because this question is irrelevant to my domain."
3. If the context does not contain specific information to answer a relevant question, respond with "I don't know"
4. Do not use any external knowledge or make assumptions

QUESTION: {question}

ANSWER:
    """,
    input_variables=['context', 'question']
)

# # Create retriever from vector store
# retriever = vector_store.as_retriever(
#     search_type="similarity",
#     search_kwargs={"k": 4}
# )

# Query the retriever
question = "who is brock lasner?"
retrieved_docs = retriever.invoke(question)

retrieved_docs

context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
context_text

final_prompt = prompt.invoke({"context": context_text, "question": question})

final_prompt

"""# Step 4 - Generation

# GROQ api key token and model
"""

answer = client.chat.completions.create(
    model="llama-3.3-70b-versatile",
    messages=[{"role": "user", "content": final_prompt.text}],
    max_tokens=1000 # You can adjust max_tokens as needed
)
print(answer.choices[0].message.content)

"""# CHAINLIT UI

"""

!pip install streamlit groq youtube-transcript-api langchain-community \
    faiss-cpu tiktoken python-dotenv sentence-transformers

!pip install streamlit pyngrok

!ngrok authtoken 34N1wfrr8sPhrdt14O6i49tyOaf_4CjJheXNU4TKJefrkYpFY

from pyngrok import ngrok
import time

# Kill any existing ngrok tunnels
ngrok.kill()

# Start streamlit in background
!streamlit run app.py &>/dev/null&

# Wait for streamlit to start
time.sleep(5)

# Create public URL
public_url = ngrok.connect(8501)
print("=" * 50)
print(f"ğŸŒ Your app is live at: {public_url}")
print("=" * 50)
print("ğŸ“± Click the link above to access your app!")
print("âš ï¸  Keep this cell running to keep the app alive")
